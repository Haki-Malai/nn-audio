{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python363jvsc74a57bd04c622b718bf9e061ea1a692dd46a8bb5928d3145b3a402924c30e011fa7f6cdc",
   "display_name": "Python 3.6.3 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "4c622b718bf9e061ea1a692dd46a8bb5928d3145b3a402924c30e011fa7f6cdc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Loading Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\HakiM\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pydub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.io.wavfile import read, write"
   ]
  },
  {
   "source": [
    "# Loading the dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#training dataset\n",
    "train_path  = '../dataset/wavLITE/train/'\n",
    "errors = []\n",
    "complete = 0\n",
    "dataset = []\n",
    "labels = []\n",
    "for filename in os.listdir(train_path+'true/'):\n",
    "    try:\n",
    "        fs, y = read(train_path+'true/'+filename)\n",
    "        dataset.append(np.array(y[5000000:5100000]))\n",
    "        labels.append(1)\n",
    "        complete = complete + 1\n",
    "        if len(y) < 10000000:\n",
    "            print(filename)\n",
    "    except:\n",
    "        errors.append(filename)\n",
    "    print('Complete: ',(complete*100)/len(os.listdir(train_path+'true/')),'%')\n",
    "\n",
    "complete = 0\n",
    "for filename in os.listdir(train_path+'false/'):\n",
    "    try:\n",
    "        fs, y = read(train_path+'false/'+filename)\n",
    "        dataset.append(np.array(y[5000000:5100000]))\n",
    "        labels.append(1)\n",
    "        complete = complete + 1\n",
    "        if len(y) < 10000000:\n",
    "            print(filename)\n",
    "    except:\n",
    "        errors.append(filename)\n",
    "    print('Complete: ',(complete*100)/len(os.listdir(train_path+'false/')),'%')\n",
    "\n",
    "print(\"Script complete with \", len(errors), \" errors.\")\n",
    "if len(errors):\n",
    "    for error in errors:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset\n",
    "test_data = []\n",
    "test_labels = []\n",
    "test_path  = '../dataset/wavLITE/test/'\n",
    "\n",
    "complete = 0\n",
    "for filename in os.listdir(test_path+'true/'):\n",
    "    try:\n",
    "        fs, y = read(test_path+'true/'+filename)\n",
    "        test_data.append(np.array(y[5000000:5100000]))\n",
    "        test_labels.append(1)\n",
    "        complete = complete + 1\n",
    "    except:\n",
    "        errors.append(filename)\n",
    "    print('Complete: ',(complete*100)/len(os.listdir(test_path+'true/')),'%')\n",
    "\n",
    "complete = 0\n",
    "for filename in os.listdir(test_path+'false/'):\n",
    "    try:\n",
    "        fs, y = read(test_path+'false/'+filename)\n",
    "        test_data.append(np.array(y[5000000:5100000]))\n",
    "        test_labels.append(0)\n",
    "        complete = complete + 1\n",
    "    except:\n",
    "        errors.append(filename)\n",
    "    print('Complete: ',(complete*100)/len(os.listdir(test_path+'false/')),'%')\n",
    "\n",
    "print(\"Script complete with \", len(errors), \" errors.\")\n",
    "if len(errors):\n",
    "    for error in errors:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert dataset list of numpy arrays to list of tensors\n",
    "for i in range(len(dataset)):    \n",
    "    dataset[i] = tf.convert_to_tensor(dataset[i])\n",
    "    labels[i] = tf.convert_to_tensor(dataset[i])\n",
    "\n",
    "for i in range(len(test_data)):    \n",
    "    test_data[i] = tf.convert_to_tensor(test_data[i])\n",
    "    test_labels[i] = tf.convert_to_tensor(dataset[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_data)):\n",
    "    if len(test_data[i]) != len(test_data[i]):\n",
    "        print(len(test_data[i]) , len(test_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((dataset, labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1000000, 2)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}